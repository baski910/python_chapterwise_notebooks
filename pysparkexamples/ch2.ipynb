{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac054f53-58b9-42ba-a695-b8ae778d9e84",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Understanding PySpark Architecture\n",
    "</h2>\n",
    "<p>\n",
    "PySpark, as the Python API for Apache Spark, abstracts the complexities of distributed computing while enabling seamless integration with Python's rich ecosystem\n",
    "</p>\n",
    "<p>\n",
    "At its core, PySpark operates in a distributed environment, orchestrating computations across multiple nodes.\n",
    "</p>\n",
    "<p>\n",
    "Understanding the cluster setup is key to leveraging PySpark's capabilities:\n",
    "</p>\n",
    "<h2>Components of a Cluster</h2>\n",
    "<img src=\"architecture.png\" width=\"200\" height=\"200\">\n",
    "<ol>\n",
    "<li>\n",
    "<strong>\n",
    "Driver Program:\n",
    "</strong>\n",
    "</li>\n",
    "<li>\n",
    "<strong>\n",
    "Cluster Manager:\n",
    "</strong>\n",
    "</li>\n",
    "<li>\n",
    "<strong>\n",
    "Worker Nodes:\n",
    "</strong>\n",
    "</li>\n",
    "<li>\n",
    "<strong>\n",
    "Executors:\n",
    "</strong>\n",
    "</li>\n",
    "</ol>\n",
    "<p>\n",
    "<strong>Driver Program</strong>\n",
    "The entry point for the PySpark application. It runs the user's code, creates the SparkSession and SparkContext, and coordinates the overall workflow by converting the code into an execution plan (DAG) and scheduling tasks\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "Cluster Manager:\n",
    "</strong>\n",
    "An external service responsible for acquiring and allocating resources (CPU, memory) on the cluster's worker nodes. Examples include Spark's own Standalone manager, Hadoop YARN, Apache Mesos, or Kubernetes.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "Worker Nodes:\n",
    "</strong> \n",
    "The physical or virtual machines in the cluster that run the actual computations.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "Executors: \n",
    "</strong>\n",
    "Processes that run on each worker node and are responsible for executing the tasks assigned by the driver program. They also store intermediate results in memory or on disk.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ee3c1-22ef-436b-b346-6b91952d9983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
