{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bead9f-2dc5-4191-a4a6-65d1cba956ac",
   "metadata": {},
   "source": [
    "<h2>PySpark DataFrame</h2>\n",
    "<h2>1. Create Empty RDD in PySpark</h2>\n",
    "<p>Create an <strong>empty RDD</strong> by using emptyRDD() of SparkContext for example <strong>spark.sparkContext.emptyRDD()</strong></p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "from pyspark.sql import SparkSession</br>\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()</br>\n",
    "\n",
    "#Creates Empty RDD</br>\n",
    "emptyRDD = spark.sparkContext.emptyRDD()</br>\n",
    "print(emptyRDD)</br>\n",
    "\n",
    "#Diplays</br>\n",
    "#EmptyRDD[188] at emptyRDD</br>\n",
    "</p>\n",
    "<p>Alternatively you can also get empty RDD by using spark.sparkContext.parallelize([]).</p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "#Creates Empty RDD using parallelize</br>\n",
    "rdd2= spark.sparkContext.parallelize([])</br>\n",
    "print(rdd2)</br>\n",
    "\n",
    "#EmptyRDD[205] at emptyRDD at NativeMethodAccessorImpl.java:0</br>\n",
    "#ParallelCollectionRDD[206] at readRDDFromFile at PythonRDD.scala:262</br>\n",
    "</p>\n",
    "<h2>2. Create Empty DataFrame with Schema (StructType)</h2>\n",
    "<p>Here is an example for creating empty DataFrame with Schema</p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "#Create Schema</br>\n",
    "from pyspark.sql.types import StructType,StructField, StringType</br>\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),</br>\n",
    "  StructField('middlename', StringType(), True),</br>\n",
    "  StructField('lastname', StringType(), True)</br>\n",
    "  ])</br>\n",
    "</p>\n",
    "<p>Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "#Create empty DataFrame from empty RDD</br>\n",
    "df = spark.createDataFrame(emptyRDD,schema)</br>\n",
    "df.printSchema()</br>\n",
    "</p>\n",
    "<h2>3. Convert Empty RDD to DataFrame</h2>\n",
    "<p>You can also create empty DataFrame by converting empty RDD to DataFrame using toDF().</p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "#Convert empty RDD to Dataframe</br>\n",
    "df1 = emptyRDD.toDF(schema)</br>\n",
    "df1.printSchema()</br>\n",
    "\n",
    "#Create empty DataFrame directly.</br>\n",
    "df2 = spark.createDataFrame([], schema)</br>\n",
    "df2.printSchema()</br>\n",
    "</p>\n",
    "########### comes in next section ############\n",
    "<h2>Create DataFrame from RDD</h2>\n",
    "<h2>1. Create PySpark RDD</h2>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "from pyspark.sql import SparkSession</br>\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()</br>\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]</br>\n",
    "rdd = spark.sparkContext.parallelize(dept)</br>\n",
    "</p>\n",
    "<h2>2. Convert PySpark RDD to DataFrame</h2>\n",
    "<p>Converting PySpark RDD to DataFrame can be done using <strong>toDF()</strong>, <strong>createDataFrame()</strong></p>\n",
    "<p height=\"200\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "<h2>Using rdd.toDF() function</h2></br>\n",
    "df = rdd.toDF()</br>\n",
    "df.printSchema()</br>\n",
    "df.show(truncate=False)</br>\n",
    "<h2>Using PySpark createDataFrame() function</h2></br>\n",
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)</br>\n",
    "deptDF.printSchema()</br>\n",
    "deptDF.show(truncate=False)</br>\n",
    "<h2>Using createDataFrame() with StructType schema</h2></br>\n",
    "from pyspark.sql.types import StructType,StructField, StringType</br>\n",
    "deptSchema = StructType([  </br>     \n",
    "    StructField('dept_name', StringType(), True),</br>\n",
    "    StructField('dept_id', StringType(), True)</br>\n",
    "])</br>\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)</br>\n",
    "deptDF1.printSchema()</br>\n",
    "deptDF1.show(truncate=False)</br>\n",
    "</p>\n",
    "\n",
    "<p height=\"400\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "<h2>Complete Example</h2>\n",
    "import pyspark</br>\n",
    "from pyspark.sql import SparkSession</br>\n",
    "</br>\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()</br>\n",
    "</br>\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]</br>\n",
    "rdd = spark.sparkContext.parallelize(dept)</br>\n",
    "</br>\n",
    "df = rdd.toDF()</br>\n",
    "df.printSchema()</br>\n",
    "df.show(truncate=False)</br>\n",
    "</br>\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]</br>\n",
    "df2 = rdd.toDF(deptColumns)</br>\n",
    "df2.printSchema()</br>\n",
    "df2.show(truncate=False)</br>\n",
    "</br>\n",
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)</br>\n",
    "deptDF.printSchema()</br>\n",
    "deptDF.show(truncate=False)</br>\n",
    "</br>\n",
    "from pyspark.sql.types import StructType,StructField, StringType</br>\n",
    "deptSchema = StructType([</br>       \n",
    "    StructField('dept_name', StringType(), True),</br>\n",
    "    StructField('dept_id', StringType(), True)</br>\n",
    "])</br>\n",
    "</br>\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)</br>\n",
    "deptDF1.printSchema()</br>\n",
    "deptDF1.show(truncate=False)</br>\n",
    "</p>\n",
    "############ third section #################\n",
    "<h2>Convert PySpark DataFrame to Pandas</h2>\n",
    "<p>Use the toPandas() method available in PySpark DataFrame objects to convert them to DataFrames.<p>\n",
    "<p>Pandas DataFrames are in-memory data structures, so consider memory constraints when converting large PySpark DataFrames.</p>\n",
    "<p>Converting PySpark DataFrames to Pandas DataFrames allows you to leverage Pandas’ extensive functionality for data manipulation and analysis.</p>\n",
    "<p height=\"200\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "import pyspark</br>\n",
    "from pyspark.sql import SparkSession</br>\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()</br>\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),</br>\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),</br>\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),</br>\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),</br>\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]</br>\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]</br>\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)</br>\n",
    "pysparkDF.printSchema()</br>\n",
    "pysparkDF.show(truncate=False)</br>\n",
    "</p>\n",
    "<p><strong>toPandas()</strong> results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data. running on larger dataset’s results in memory error and crashes the application.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "pandasDF = pysparkDF.toPandas()</br>\n",
    "print(pandasDF)</br>\n",
    "</p>\n",
    "<p>You can <strong>rename</strong> pandas columns by using rename() function.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "pandasDF = pysparkDF.toPandas()</br>\n",
    "print(pandasDF)</br>\n",
    "</p>\n",
    "<h2>Convert Spark Nested Struct DataFrame to Pandas</h2>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Nested structure elements</br>\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType</br>\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\</br>\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\</br>\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\</br>\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\</br>\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\</br>\n",
    "]</br>\n",
    "</p>\n",
    "<p height=\"150\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "schemaStruct = StructType([</br>\n",
    "        StructField('name', StructType([</br>\n",
    "             StructField('firstname', StringType(), True),</br>\n",
    "             StructField('middlename', StringType(), True),</br>\n",
    "             StructField('lastname', StringType(), True)</br>\n",
    "             ])),</br>\n",
    "          StructField('dob', StringType(), True),</br>\n",
    "         StructField('gender', StringType(), True),</br>\n",
    "         StructField('salary', StringType(), True)</br>\n",
    "         ])</br>\n",
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)</br>\n",
    "df.printSchema()</br>\n",
    "pandasDF2 = df.toPandas()</br>\n",
    "print(pandasDF2)</br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9af31-eb12-4ebf-9816-319c4fbfdd3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
