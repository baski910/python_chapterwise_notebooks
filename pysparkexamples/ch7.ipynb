{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b97c5e-a0f3-4303-815f-f608153ff1ff",
   "metadata": {},
   "source": [
    "<h2>RDD Creation</h2>\n",
    "<p>\n",
    "You can create RDD by parallelizing the existing collection and reading data from a disk.</br>\n",
    "<ul>\n",
    "<li>\n",
    "parallelizing an existing collection\n",
    "</li>\n",
    "<li>\n",
    "referencing a dataset in an external storage system (HDFS, S3 and many more). \n",
    "</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "Initialize SparkSession using the builder pattern method defined in SparkSession class.\n",
    "</strong>\n",
    "</p>\n",
    "<p height=\"200\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Imports</br>\n",
    "from pyspark.sql import SparkSession</br>\n",
    "</br>\n",
    "# Create SparkSession</br>\n",
    "spark = SparkSession.builder</br>\n",
    "      .master(\"local[1]\")</br>\n",
    "      .appName(\"SparkByExamples.com\")</br>\n",
    "      .getOrCreate()</br>   \n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "master()\n",
    "</strong>\n",
    "If you are running it on the cluster you need to use your master name as an argument to master().</br> usually, it would be either yarn (Yet Another Resource Negotiator) or mesos depends on your cluster setup.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "local[x]:\n",
    "</strong>\n",
    "When operating in Standalone mode, specify ‘local[x]’, where ‘x’ is an integer greater than 0, to determine the number of partitions for RDDs.</br> Ideally, set ‘x’ to match the number of CPU cores available on your system for optimal performance.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "appName():\n",
    "</strong>\n",
    "Used to set your application name.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "getOrCreate():\n",
    "</strong>\n",
    "This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "</p>\n",
    "<h2>Using sparkContext.parallelize()</h2>\n",
    "<p>\n",
    "<strong>\n",
    "parallelize() \n",
    "</strong> function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD.</br> This method of creating an RDD is used when you already have data in memory that is either loaded from a file or from a database. and all data must be present in the driver program prior to creating RDD.\n",
    "</p>\n",
    "<img width=\"200\" height=\"300\" src=\"parallelize.png\">\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create RDD from parallelize</br>  \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]</br>\n",
    "rdd = spark.sparkContext.parallelize(data)</br>\n",
    "</p>\n",
    "<h2>Using sparkContext.textFile()</h2>\n",
    "<p>\n",
    "Use the textFile() method to read a .txt file into RDD.\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create RDD from external Data source</br>\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")</br>\n",
    "</p>\n",
    "<h2>Using sparkContext.wholeTextFiles()</h2>\n",
    "<p>\n",
    "<strong>wholeTextFiles()</strong> function returns a PairRDD with the key being the file path and the value being file content.\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Read entire file into a RDD as single record.</br>\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")</br>\n",
    "</p>\n",
    "<p>\n",
    "Using <strong>emptyRDD()</strong> method on sparkContext we can create an RDD with no data. This method creates an empty RDD with no partition.\n",
    "</p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create an empty RDD with no partition</br> \n",
    "rdd = spark.sparkContext.emptyRDD()</br>\n",
    "</br>\n",
    "# Output:</br>\n",
    "# rddString = spark.sparkContext.emptyRDD[String]</br>\n",
    "</p>\n",
    "<h2>Creating empty RDD with partition</h2>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e555dc5-d38d-4779-b90c-10823bfe73ed",
   "metadata": {},
   "source": [
    "<h2>PySpark RDD Operations</h2>\n",
    "<p>\n",
    "RDD operations are the core transformations and actions performed on RDDs\n",
    "</p>\n",
    "<p>\n",
    "<strong>RDD transformations</strong> – Transformations are lazy operations; instead of updating an RDD, these operations return another RDD.\n",
    "<strong>RDD actions</strong> – operations that trigger computation and return RDD values.\n",
    "</p>\n",
    "<h2>RDD Transformations</h2>\n",
    "<p>\n",
    "<strong>Transformations on PySpark RDD</strong> return another RDD, and transformations are lazy, meaning they don’t execute until you call an action on RDD.</br>Some transformations on RDDs are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return a new RDD instead of updating the current.\n",
    "</p>\n",
    "<p>\n",
    "Let’s do transformations to perform word count example.\n",
    "</p>\n",
    "<p>\n",
    "Read the text file into RDD\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Read data from text file</br>\n",
    "rdd = spark.sparkContext.textFile(\"test.txt\")</br>\n",
    "</p>\n",
    "<p><strong>flatMap()</strong> transformation in the RDD API flattens the resulting RDD after applying a function to each element, producing a new RDD.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# split the data by space and flatten it.</br>\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))</br>\n",
    "</p>\n",
    "<p>\n",
    "<strong>map()</strong> transformation is used to perform various complex operations, such as adding or updating an element.\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Apply the mpa() transformation</br>\n",
    "# Add a new element with value 1 to each word</br>\n",
    "rdd3 = rdd2.map(lambda x: (x,1))</br>\n",
    "</p>\n",
    "<p>\n",
    "<strong>reduceByKey()</strong> combines the values associated with each key using the provided function. In our scenario, it aggregates the word strings by using the sum function on the corresponding values\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Use reduceByKey()</br>\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)</br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8148ab0-103e-4693-88c6-3c1f9eae23f0",
   "metadata": {},
   "source": [
    "<h2>RDD Actions</h2>\n",
    "\n",
    "<p><strong>RDD Action operations</strong> trigger the execution of transformations on RDDs (Resilient Distributed Datasets) and produce a result that can be either returned to the driver program or saved to an external storage system.\n",
    "</p>\n",
    "<p><strong>count</sytong> - Returns the number of records in the RDD</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Action - count</br>\n",
    "print(\"Count : \"+str(rdd6.count()))</br>\n",
    "</p>\n",
    "<p><strong>first()</sytong> - Returns the first record</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Action - first</br>\n",
    "firstRec = rdd6.first()</br>\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])</br>\n",
    "</p>\n",
    "<p><strong>max()</sytong> - Returns max record.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Action - max</br>\n",
    "datMax = rdd6.max()</br>\n",
    "print(\"Max Record : \"+str(datMax[0]) + \",\"+ datMax[1])</br>\n",
    "</p>\n",
    "<p><strong>reduce()</sytong> – Reduces the records to single, we can use this to count or sum.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Action - reduce</br>\n",
    "totalWordCount = rdd6.reduce(lambda a,b: (a[0]+b[0],a[1]))</br>\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))</br>\n",
    "</p>\n",
    "<p><strong>take()</sytong> – Returns the record specified as an argument.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Action - take</br>\n",
    "data3 = rdd6.take(3)</br>\n",
    "for f in data3:</br>\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])</br>\n",
    "</p>\n",
    "<p><strong>collect()</sytong>  – Returns all data from RDD as an array. Be careful when you use this action when you are working with huge RDD with millions and billions of data as you may run out of memory on the driver.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "</br>\n",
    "# Action - collect</br>\n",
    "data = rdd6.collect()</br>\n",
    "for f in data:</br>\n",
    "    print(\"Key:\"+ str(f[0]) +\", Value:\"+f[1])</br>\n",
    "</p>\n",
    "<p><strong>saveAsTextFile()</sytong> – Using saveAsTestFile action, we can write the RDD to a text file.</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "</br>\n",
    "# Action - collect</br>\n",
    "data = rdd6.collect()</br>\n",
    "for f in data:</br>\n",
    "    print(\"Key:\"+ str(f[0]) +\", Value:\"+f[1])</br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651bb59-d2da-4b29-961c-6deca24de183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
