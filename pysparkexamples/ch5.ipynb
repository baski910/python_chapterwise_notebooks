{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2858316c-8dc2-4606-ad52-021f4ec98065",
   "metadata": {},
   "source": [
    "<h2>RDD Creation</h2>\n",
    "<p>\n",
    "You can create RDD by parallelizing the existing collection and reading data from a disk.</br>\n",
    "<ul>\n",
    "<li>\n",
    "parallelizing an existing collection\n",
    "</li>\n",
    "<li>\n",
    "referencing a dataset in an external storage system (HDFS, S3 and many more). \n",
    "</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "Initialize SparkSession using the builder pattern method defined in SparkSession class.\n",
    "</strong>\n",
    "</p>\n",
    "<p height=\"200\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Imports</br>\n",
    "from pyspark.sql import SparkSession</br>\n",
    "</br>\n",
    "# Create SparkSession</br>\n",
    "spark = SparkSession.builder</br>\n",
    "      .master(\"local[1]\")</br>\n",
    "      .appName(\"SparkByExamples.com\")</br>\n",
    "      .getOrCreate()</br>   \n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "master()\n",
    "</strong>\n",
    "If you are running it on the cluster you need to use your master name as an argument to master().</br> usually, it would be either yarn (Yet Another Resource Negotiator) or mesos depends on your cluster setup.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "local[x]:\n",
    "</strong>\n",
    "When operating in Standalone mode, specify ‘local[x]’, where ‘x’ is an integer greater than 0, to determine the number of partitions for RDDs.</br> Ideally, set ‘x’ to match the number of CPU cores available on your system for optimal performance.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "appName():\n",
    "</strong>\n",
    "Used to set your application name.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "getOrCreate():\n",
    "</strong>\n",
    "This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "</p>\n",
    "<h2>Using sparkContext.parallelize()</h2>\n",
    "<p>\n",
    "<strong>\n",
    "parallelize() \n",
    "</strong> function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD.</br> This method of creating an RDD is used when you already have data in memory that is either loaded from a file or from a database. and all data must be present in the driver program prior to creating RDD.\n",
    "</p>\n",
    "<img width=\"200\" height=\"300\" src=\"parallelize.png\">\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create RDD from parallelize</br>  \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]</br>\n",
    "rdd = spark.sparkContext.parallelize(data)</br>\n",
    "</p>\n",
    "<h2>Using sparkContext.textFile()</h2>\n",
    "<p>\n",
    "Use the textFile() method to read a .txt file into RDD.\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create RDD from external Data source</br>\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")</br>\n",
    "</p>\n",
    "<h2>Using sparkContext.wholeTextFiles()</h2>\n",
    "<p>\n",
    "<strong>wholeTextFiles()</strong> function returns a PairRDD with the key being the file path and the value being file content.\n",
    "</p>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Read entire file into a RDD as single record.</br>\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")</br>\n",
    "</p>\n",
    "<p>\n",
    "Using <strong>emptyRDD()</strong> method on sparkContext we can create an RDD with no data. This method creates an empty RDD with no partition.\n",
    "</p>\n",
    "<p height=\"100\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create an empty RDD with no partition</br> \n",
    "rdd = spark.sparkContext.emptyRDD()</br>\n",
    "\n",
    "# Output:</br>\n",
    "# rddString = spark.sparkContext.emptyRDD[String]</br>\n",
    "</p>\n",
    "<h2>Creating empty RDD with partition</h2>\n",
    "<p height=\"50\" width=\"100%\" style=\"background:black;font-size:20px;color:white\">\n",
    "# Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed79a5-54f5-4f7b-a602-90a1fa4d78cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
