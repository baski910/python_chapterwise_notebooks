{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3295c4c-e499-40a8-92de-aa7a135f1068",
   "metadata": {},
   "source": [
    "<h2>RDD - Resilient Distributed Dataset</h2>\n",
    "<p>RDD is a fault-tolerant, immutable, distributed collection of objects. </p>\n",
    "<p>Immutable means that once you create an RDD, you cannot change it.</p>\n",
    "<p>The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.</p>\n",
    "<h2>PySpark RDD Benefits</h2>\n",
    "<p>\n",
    "<strong>\n",
    "1. In-Memory Processing\n",
    "</strong> \n",
    "PySpark loads the data from disk and processes it in memory, and keeps the data in memory; this is the main difference between PySpark and MapReduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.\n",
    "</p>\n",
    "<p>\n",
    "<strong>2. Immutability</strong>\n",
    "PySpark RDDs are immutable in nature meaning, once RDDs are created you cannot modify them. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
    "</p>\n",
    "<p>\n",
    "<strong>3. Fault Tolerance</strong>\n",
    "PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c. Hence, if any RDD operation fails, it automatically reloads the data from other partitions.</br> Also, when PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "4. Lazy Evolution\n",
    "</strong>\n",
    "PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "</p>\n",
    "<p>\n",
    "<strong>\n",
    "5. Partitioning\n",
    "</strong>\n",
    "When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f7187-a76e-4248-8bfb-f4667248ab34",
   "metadata": {},
   "source": [
    "<h2>PySpark RDD Limitations</h2>\n",
    "<p>\n",
    "PySpark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application.</br> For these applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases.</br> The goal of RDD is to provide an efficient programming model for batch analytics and leave these asynchronous applications.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e625189-0f7a-4bc0-96f1-0e573860d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
